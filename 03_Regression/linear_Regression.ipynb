{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Linear Regression: Teaching Computers to Draw Lines!\n",
    "\n",
    "**Welcome, Young Data Scientist!**\n",
    "\n",
    "Have you ever played \"connect the dots\"? Linear regression is kind of like that, but instead of connecting dots exactly, we're teaching a computer to draw the **best possible line** through a bunch of scattered dots!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What You'll Learn in This Notebook\n",
    "\n",
    "1. **What is Linear Regression?** - Understanding the basics\n",
    "2. **Simple Linear Regression** - Drawing the best line through points\n",
    "3. **Gradient Descent** - How computers learn to improve\n",
    "4. **Polynomial Regression** - When lines aren't enough (curves!)\n",
    "5. **Regularization** - Preventing our model from being too \"creative\"\n",
    "\n",
    "---\n",
    "\n",
    "## üé® Real-World Examples of Linear Regression\n",
    "\n",
    "Linear regression helps us answer questions like:\n",
    "- If I study more hours, will my test score go up?\n",
    "- If a house is bigger, will it cost more?\n",
    "- If I water my plant more, will it grow taller?\n",
    "\n",
    "Let's dive in! üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Setting Up Our Workshop üõ†Ô∏è\n",
    "\n",
    "Before we start, we need to import some helpful tools (libraries). Think of these as our art supplies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import all the tools we need\n",
    "# Don't worry if you don't understand all of these yet!\n",
    "\n",
    "import numpy as np                    # For math operations (like a super calculator)\n",
    "import pandas as pd                   # For organizing data (like a spreadsheet)\n",
    "import matplotlib.pyplot as plt       # For drawing graphs (our art canvas)\n",
    "from matplotlib.animation import FuncAnimation  # For animations\n",
    "from IPython.display import HTML      # For showing animations in notebooks\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')     # Hide warning messages to keep things clean\n",
    "\n",
    "# Make our graphs look nice and big\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set a random seed so everyone gets the same results\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All tools are ready! Let's learn about Linear Regression!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: What is Linear Regression? ü§î\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "Imagine you're trying to predict how tall a plant will grow based on how much water you give it. You've measured several plants and written down the results. Now you want to **find a pattern** so you can predict how tall a NEW plant will grow!\n",
    "\n",
    "**Linear Regression** is a method that finds the **best straight line** through your data points.\n",
    "\n",
    "## The Equation of a Line\n",
    "\n",
    "Remember from math class? A line can be written as:\n",
    "\n",
    "$$y = mx + b$$\n",
    "\n",
    "Where:\n",
    "- **y** = what we want to predict (like plant height)\n",
    "- **x** = what we use to predict (like amount of water)\n",
    "- **m** = the **slope** (how steep the line is)\n",
    "- **b** = the **y-intercept** (where the line crosses the y-axis)\n",
    "\n",
    "In machine learning, we usually write this as:\n",
    "\n",
    "$$\\hat{y} = w \\cdot x + b$$\n",
    "\n",
    "Where:\n",
    "- **≈∑** (y-hat) = our **prediction**\n",
    "- **w** = **weight** (same as slope)\n",
    "- **b** = **bias** (same as y-intercept)\n",
    "\n",
    "Let's visualize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what different lines look like!\n",
    "\n",
    "# Create a figure with 3 subplots side by side\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# X values from 0 to 10\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "# Plot 1: Different slopes (w values)\n",
    "axes[0].plot(x, 1*x + 2, 'b-', linewidth=2, label='w=1 (gentle slope)')\n",
    "axes[0].plot(x, 2*x + 2, 'g-', linewidth=2, label='w=2 (medium slope)')\n",
    "axes[0].plot(x, 3*x + 2, 'r-', linewidth=2, label='w=3 (steep slope)')\n",
    "axes[0].set_xlabel('x (Input)', fontsize=12)\n",
    "axes[0].set_ylabel('y (Output)', fontsize=12)\n",
    "axes[0].set_title('üèîÔ∏è Different Slopes (w)\\n(All have b=2)', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 35)\n",
    "\n",
    "# Plot 2: Different y-intercepts (b values)\n",
    "axes[1].plot(x, 2*x + 0, 'b-', linewidth=2, label='b=0')\n",
    "axes[1].plot(x, 2*x + 5, 'g-', linewidth=2, label='b=5')\n",
    "axes[1].plot(x, 2*x + 10, 'r-', linewidth=2, label='b=10')\n",
    "axes[1].set_xlabel('x (Input)', fontsize=12)\n",
    "axes[1].set_ylabel('y (Output)', fontsize=12)\n",
    "axes[1].set_title('üìç Different Y-Intercepts (b)\\n(All have w=2)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim(0, 35)\n",
    "\n",
    "# Plot 3: Positive vs Negative slopes\n",
    "axes[2].plot(x, 2*x + 5, 'g-', linewidth=2, label='w=+2 (going UP ‚ÜóÔ∏è)')\n",
    "axes[2].plot(x, -2*x + 25, 'r-', linewidth=2, label='w=-2 (going DOWN ‚ÜòÔ∏è)')\n",
    "axes[2].axhline(y=15, color='blue', linestyle='--', linewidth=2, label='w=0 (flat ‚Üí)')\n",
    "axes[2].set_xlabel('x (Input)', fontsize=12)\n",
    "axes[2].set_ylabel('y (Output)', fontsize=12)\n",
    "axes[2].set_title('‚ÜóÔ∏è Positive vs Negative Slopes', fontsize=14)\n",
    "axes[2].legend()\n",
    "axes[2].set_ylim(0, 35)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('01_understanding_lines.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° KEY INSIGHT:\")\n",
    "print(\"   ‚Ä¢ The SLOPE (w) controls how STEEP the line is\")\n",
    "print(\"   ‚Ä¢ The INTERCEPT (b) controls WHERE the line starts on the y-axis\")\n",
    "print(\"   ‚Ä¢ Linear Regression finds the BEST w and b for our data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Our First Dataset - Study Hours vs Test Scores üìù\n",
    "\n",
    "Let's create a simple, relatable dataset: **How do study hours affect test scores?**\n",
    "\n",
    "This is something every student can relate to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create our dataset!\n",
    "# We'll pretend we surveyed 30 students about their study habits\n",
    "\n",
    "# Number of students in our survey\n",
    "n_students = 30\n",
    "\n",
    "# Study hours (randomly between 0.5 and 8 hours)\n",
    "study_hours = np.random.uniform(0.5, 8, n_students)\n",
    "\n",
    "# Test scores - generally, more study = higher score, but with some randomness\n",
    "# True relationship: score = 40 + 7*hours (plus some random noise)\n",
    "true_intercept = 40  # Base score even with minimal studying\n",
    "true_slope = 7       # Each hour of studying adds ~7 points\n",
    "noise = np.random.normal(0, 5, n_students)  # Random variation (life isn't perfect!)\n",
    "\n",
    "test_scores = true_intercept + true_slope * study_hours + noise\n",
    "\n",
    "# Make sure scores stay between 0 and 100\n",
    "test_scores = np.clip(test_scores, 0, 100)\n",
    "\n",
    "# Create a nice DataFrame (like a spreadsheet) to view our data\n",
    "data = pd.DataFrame({\n",
    "    'Student': [f'Student {i+1}' for i in range(n_students)],\n",
    "    'Study_Hours': np.round(study_hours, 1),\n",
    "    'Test_Score': np.round(test_scores, 1)\n",
    "})\n",
    "\n",
    "print(\"üìä Our Dataset: Study Hours vs Test Scores\")\n",
    "print(\"=\" * 45)\n",
    "print(data.head(10).to_string(index=False))\n",
    "print(f\"\\n... and {n_students - 10} more students\")\n",
    "print(f\"\\nüìà Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Average study hours: {study_hours.mean():.1f} hours\")\n",
    "print(f\"   ‚Ä¢ Average test score: {test_scores.mean():.1f} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize our data!\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot each student as a dot\n",
    "plt.scatter(study_hours, test_scores, \n",
    "            s=100,                    # Size of dots\n",
    "            c='royalblue',           # Color\n",
    "            alpha=0.7,               # Slight transparency\n",
    "            edgecolors='darkblue',   # Edge color\n",
    "            linewidth=1.5)\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('üìö Study Hours', fontsize=14)\n",
    "plt.ylabel('üìù Test Score', fontsize=14)\n",
    "plt.title('üéì Does Studying More Lead to Higher Test Scores?', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Add a grid for easier reading\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim(0, 9)\n",
    "plt.ylim(30, 105)\n",
    "\n",
    "plt.savefig('02_scatter_plot_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç OBSERVATION:\")\n",
    "print(\"   Can you see a pattern? It looks like students who study more\")\n",
    "print(\"   tend to get higher scores! But the points don't form a perfect line.\")\n",
    "print(\"   \\n   Our goal: Find the BEST line that represents this pattern!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Finding the Best Line üéØ\n",
    "\n",
    "## What Makes a Line \"Good\" or \"Bad\"?\n",
    "\n",
    "To find the **best** line, we need a way to measure how **good** or **bad** a line is. We do this by looking at the **errors** (also called **residuals**).\n",
    "\n",
    "**Error** = Actual Value - Predicted Value\n",
    "\n",
    "The **best line** is the one that makes these errors as **small as possible**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize what \"errors\" look like!\n",
    "\n",
    "# We'll try a \"guess\" line and see the errors\n",
    "guess_slope = 5      # Our guess for slope\n",
    "guess_intercept = 45 # Our guess for intercept\n",
    "\n",
    "# Calculate predictions using our guess\n",
    "predictions = guess_slope * study_hours + guess_intercept\n",
    "\n",
    "# Create the plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Left plot: Show the errors as vertical lines\n",
    "axes[0].scatter(study_hours, test_scores, s=100, c='royalblue', \n",
    "                alpha=0.7, edgecolors='darkblue', linewidth=1.5, label='Actual Scores')\n",
    "axes[0].plot(study_hours, predictions, 'r-', linewidth=2, label='Our Guess Line')\n",
    "\n",
    "# Draw error lines (from actual point to prediction on line)\n",
    "for i in range(len(study_hours)):\n",
    "    axes[0].plot([study_hours[i], study_hours[i]], \n",
    "                 [test_scores[i], predictions[i]], \n",
    "                 'g--', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "# Add a sample error annotation\n",
    "idx = 5\n",
    "axes[0].annotate('Error\\n(Actual - Predicted)', \n",
    "                 xy=(study_hours[idx], (test_scores[idx] + predictions[idx])/2),\n",
    "                 xytext=(study_hours[idx]+1.5, (test_scores[idx] + predictions[idx])/2),\n",
    "                 fontsize=11, ha='left',\n",
    "                 arrowprops=dict(arrowstyle='->', color='green'))\n",
    "\n",
    "axes[0].set_xlabel('üìö Study Hours', fontsize=12)\n",
    "axes[0].set_ylabel('üìù Test Score', fontsize=12)\n",
    "axes[0].set_title('üìè Errors: Distance from Points to Line', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Right plot: Good line vs Bad line comparison\n",
    "bad_slope = 3\n",
    "bad_intercept = 60\n",
    "bad_predictions = bad_slope * study_hours + bad_intercept\n",
    "\n",
    "good_slope = 7\n",
    "good_intercept = 40\n",
    "good_predictions = good_slope * study_hours + good_intercept\n",
    "\n",
    "x_line = np.linspace(0, 9, 100)\n",
    "\n",
    "axes[1].scatter(study_hours, test_scores, s=100, c='royalblue', \n",
    "                alpha=0.7, edgecolors='darkblue', linewidth=1.5)\n",
    "axes[1].plot(x_line, bad_slope * x_line + bad_intercept, 'r-', \n",
    "             linewidth=2, label=f'Bad Line (w={bad_slope}, b={bad_intercept})')\n",
    "axes[1].plot(x_line, good_slope * x_line + good_intercept, 'g-', \n",
    "             linewidth=2, label=f'Good Line (w={good_slope}, b={good_intercept})')\n",
    "\n",
    "axes[1].set_xlabel('üìö Study Hours', fontsize=12)\n",
    "axes[1].set_ylabel('üìù Test Score', fontsize=12)\n",
    "axes[1].set_title('‚úÖ Good Line vs ‚ùå Bad Line', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xlim(0, 9)\n",
    "axes[1].set_ylim(30, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('03_errors_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Calculate errors for comparison\n",
    "bad_errors = test_scores - bad_predictions\n",
    "good_errors = test_scores - good_predictions\n",
    "\n",
    "print(\"\\nüìä ERROR COMPARISON:\")\n",
    "print(f\"   ‚ùå Bad Line  - Average Absolute Error: {np.mean(np.abs(bad_errors)):.2f} points\")\n",
    "print(f\"   ‚úÖ Good Line - Average Absolute Error: {np.mean(np.abs(good_errors)):.2f} points\")\n",
    "print(\"\\n   The GOOD line has SMALLER errors - that's what we want!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Cost Function (MSE) üìâ\n",
    "\n",
    "To measure how \"bad\" a line is, we use something called a **Cost Function** or **Loss Function**.\n",
    "\n",
    "The most common one is **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "In simple terms:\n",
    "1. **Calculate each error**: (actual - predicted)\n",
    "2. **Square each error**: This makes all errors positive and punishes big errors more\n",
    "3. **Take the average**: Add them up and divide by the number of points\n",
    "\n",
    "**Lower MSE = Better Line!** üéØ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(y_actual, y_predicted):\n",
    "    \"\"\"\n",
    "    Calculate Mean Squared Error (MSE)\n",
    "    \n",
    "    This function measures how \"wrong\" our predictions are.\n",
    "    Lower MSE = Better predictions!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_actual : array of actual/true values\n",
    "    y_predicted : array of our predictions\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    MSE value (a single number)\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate errors (actual - predicted)\n",
    "    errors = y_actual - y_predicted\n",
    "    \n",
    "    # Step 2: Square each error\n",
    "    squared_errors = errors ** 2\n",
    "    \n",
    "    # Step 3: Calculate the mean (average)\n",
    "    mse = np.mean(squared_errors)\n",
    "    \n",
    "    return mse\n",
    "\n",
    "\n",
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    Make predictions using our line equation: y = wx + b\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : input values (study hours)\n",
    "    w : weight (slope)\n",
    "    b : bias (y-intercept)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Predicted values\n",
    "    \"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "\n",
    "# Let's test different lines and see their MSE!\n",
    "test_lines = [\n",
    "    (3, 60, 'Bad Line 1'),\n",
    "    (5, 50, 'Medium Line'),\n",
    "    (7, 40, 'Good Line'),\n",
    "    (true_slope, true_intercept, 'True Line (what we want to find)')\n",
    "]\n",
    "\n",
    "print(\"üî¨ COMPARING DIFFERENT LINES:\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Line':<35} {'w':>5} {'b':>5} {'MSE':>10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for w, b, name in test_lines:\n",
    "    preds = predict(study_hours, w, b)\n",
    "    mse = calculate_mse(test_scores, preds)\n",
    "    print(f\"{name:<35} {w:>5} {b:>5} {mse:>10.2f}\")\n",
    "\n",
    "print(\"\\nüí° Notice: The line closer to the TRUE values has LOWER MSE!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Gradient Descent - Teaching the Computer to Learn! üß†\n",
    "\n",
    "Now comes the exciting part! **Gradient Descent** is how the computer **learns** to find the best line.\n",
    "\n",
    "## The Mountain Analogy üèîÔ∏è\n",
    "\n",
    "Imagine you're lost on a mountain and want to get to the bottom (lowest point). It's foggy, so you can't see far. What would you do?\n",
    "\n",
    "**Strategy**: Feel which direction goes DOWN, then take a step that way. Repeat!\n",
    "\n",
    "That's exactly what gradient descent does:\n",
    "1. **Start somewhere** (with random w and b values)\n",
    "2. **Figure out which direction reduces the error** (calculate the gradient)\n",
    "3. **Take a small step in that direction** (update w and b)\n",
    "4. **Repeat** until we reach the bottom (minimum error)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the \"mountain\" of errors!\n",
    "# We'll show how MSE changes with different w and b values\n",
    "\n",
    "# Create a grid of w and b values to test\n",
    "w_values = np.linspace(-5, 15, 100)  # Test slopes from -5 to 15\n",
    "b_values = np.linspace(10, 70, 100)  # Test intercepts from 10 to 70\n",
    "\n",
    "# Calculate MSE for each combination of w and b\n",
    "W, B = np.meshgrid(w_values, b_values)\n",
    "MSE_grid = np.zeros_like(W)\n",
    "\n",
    "for i in range(len(b_values)):\n",
    "    for j in range(len(w_values)):\n",
    "        preds = predict(study_hours, W[i, j], B[i, j])\n",
    "        MSE_grid[i, j] = calculate_mse(test_scores, preds)\n",
    "\n",
    "# Create the visualization\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Left plot: 3D surface (the \"mountain\")\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(W, B, MSE_grid, cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('Weight (w)', fontsize=10)\n",
    "ax1.set_ylabel('Bias (b)', fontsize=10)\n",
    "ax1.set_zlabel('MSE (Error)', fontsize=10)\n",
    "ax1.set_title('üèîÔ∏è The \"Error Mountain\"\\n(We want to reach the BOTTOM!)', fontsize=12)\n",
    "ax1.view_init(elev=30, azim=45)\n",
    "\n",
    "# Mark the minimum point\n",
    "min_idx = np.unravel_index(np.argmin(MSE_grid), MSE_grid.shape)\n",
    "ax1.scatter([W[min_idx]], [B[min_idx]], [MSE_grid[min_idx]], \n",
    "            color='red', s=100, marker='*', label='Best Point!')\n",
    "\n",
    "# Right plot: Contour map (bird's eye view)\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(W, B, MSE_grid, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.scatter([W[min_idx]], [B[min_idx]], color='red', s=200, marker='*', \n",
    "            label='Best Point!', zorder=5)\n",
    "ax2.set_xlabel('Weight (w)', fontsize=12)\n",
    "ax2.set_ylabel('Bias (b)', fontsize=12)\n",
    "ax2.set_title('üó∫Ô∏è Bird\\'s Eye View (Contour Map)\\n(Each ring = same error level)', fontsize=12)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('04_error_landscape.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéØ The BEST point we found:\")\n",
    "print(f\"   w = {W[min_idx]:.2f} (True: {true_slope})\")\n",
    "print(f\"   b = {B[min_idx]:.2f} (True: {true_intercept})\")\n",
    "print(f\"\\n   Gradient Descent helps us find this point automatically!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Math Behind Gradient Descent üìê\n",
    "\n",
    "The **gradient** tells us which direction is \"downhill\" (reduces error).\n",
    "\n",
    "For our line equation $\\hat{y} = wx + b$, the gradients are:\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial w} = -\\frac{2}{n} \\sum_{i=1}^{n} x_i \\cdot (y_i - \\hat{y}_i)$$\n",
    "\n",
    "$$\\frac{\\partial MSE}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)$$\n",
    "\n",
    "Don't worry if the math looks scary! The key idea is:\n",
    "- **Positive gradient** ‚Üí Go DOWN (decrease the value)\n",
    "- **Negative gradient** ‚Üí Go UP (increase the value)\n",
    "\n",
    "We update our values using:\n",
    "$$w_{new} = w_{old} - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w}$$\n",
    "$$b_{new} = b_{old} - \\alpha \\cdot \\frac{\\partial MSE}{\\partial b}$$\n",
    "\n",
    "Where **Œ± (alpha)** is the **learning rate** - how big of a step we take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, learning_rate=0.01, n_iterations=1000, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform Gradient Descent to find the best line!\n",
    "    \n",
    "    This function teaches the computer to find the best w and b values\n",
    "    by repeatedly taking small steps toward lower error.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    x : input values (study hours)\n",
    "    y : actual values (test scores)\n",
    "    learning_rate : how big of a step to take (Œ±)\n",
    "    n_iterations : how many times to update\n",
    "    verbose : whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w, b : the best weight and bias we found\n",
    "    history : record of all w, b, and MSE values during training\n",
    "    \"\"\"\n",
    "    n = len(x)  # Number of data points\n",
    "    \n",
    "    # Step 1: Start with random values (or zeros)\n",
    "    w = 0  # Initial weight\n",
    "    b = 0  # Initial bias\n",
    "    \n",
    "    # Keep track of our journey\n",
    "    history = {'w': [w], 'b': [b], 'mse': []}\n",
    "    \n",
    "    # Calculate initial MSE\n",
    "    predictions = predict(x, w, b)\n",
    "    initial_mse = calculate_mse(y, predictions)\n",
    "    history['mse'].append(initial_mse)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"üöÄ Starting Gradient Descent!\")\n",
    "        print(f\"   Initial: w={w:.4f}, b={b:.4f}, MSE={initial_mse:.2f}\")\n",
    "        print(\"\\n   Training...\")\n",
    "    \n",
    "    # Step 2: Repeat many times\n",
    "    for iteration in range(n_iterations):\n",
    "        # Make predictions with current w and b\n",
    "        predictions = predict(x, w, b)\n",
    "        \n",
    "        # Calculate errors\n",
    "        errors = y - predictions\n",
    "        \n",
    "        # Calculate gradients (which direction to go)\n",
    "        gradient_w = -(2/n) * np.sum(x * errors)  # How to change w\n",
    "        gradient_b = -(2/n) * np.sum(errors)      # How to change b\n",
    "        \n",
    "        # Update w and b (take a step downhill)\n",
    "        w = w - learning_rate * gradient_w\n",
    "        b = b - learning_rate * gradient_b\n",
    "        \n",
    "        # Calculate new MSE\n",
    "        new_predictions = predict(x, w, b)\n",
    "        mse = calculate_mse(y, new_predictions)\n",
    "        \n",
    "        # Save to history\n",
    "        history['w'].append(w)\n",
    "        history['b'].append(b)\n",
    "        history['mse'].append(mse)\n",
    "        \n",
    "        # Print progress at certain points\n",
    "        if verbose and (iteration + 1) in [1, 10, 50, 100, 500, n_iterations]:\n",
    "            print(f\"   Iteration {iteration+1:>4}: w={w:.4f}, b={b:.4f}, MSE={mse:.2f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n‚úÖ Training Complete!\")\n",
    "        print(f\"   Final: w={w:.4f}, b={b:.4f}, MSE={mse:.2f}\")\n",
    "    \n",
    "    return w, b, history\n",
    "\n",
    "\n",
    "# Run gradient descent on our data!\n",
    "learned_w, learned_b, history = gradient_descent(\n",
    "    study_hours, \n",
    "    test_scores, \n",
    "    learning_rate=0.01, \n",
    "    n_iterations=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize how gradient descent learned over time!\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: MSE over iterations (the learning curve)\n",
    "axes[0].plot(history['mse'], 'b-', linewidth=2)\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('MSE (Error)', fontsize=12)\n",
    "axes[0].set_title('üìâ Learning Curve\\n(Error decreases over time!)', fontsize=14)\n",
    "axes[0].set_yscale('log')  # Log scale to see the descent better\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: How w and b changed\n",
    "ax2_twin = axes[1].twinx()\n",
    "line1 = axes[1].plot(history['w'], 'b-', linewidth=2, label='w (weight)')\n",
    "line2 = ax2_twin.plot(history['b'], 'r-', linewidth=2, label='b (bias)')\n",
    "axes[1].axhline(y=true_slope, color='blue', linestyle='--', alpha=0.5, label=f'True w={true_slope}')\n",
    "ax2_twin.axhline(y=true_intercept, color='red', linestyle='--', alpha=0.5, label=f'True b={true_intercept}')\n",
    "axes[1].set_xlabel('Iteration', fontsize=12)\n",
    "axes[1].set_ylabel('Weight (w)', fontsize=12, color='blue')\n",
    "ax2_twin.set_ylabel('Bias (b)', fontsize=12, color='red')\n",
    "axes[1].set_title('üìà Parameter Changes\\n(w and b approach true values)', fontsize=14)\n",
    "axes[1].legend(loc='upper left')\n",
    "ax2_twin.legend(loc='upper right')\n",
    "\n",
    "# Plot 3: The journey on the contour map\n",
    "contour = axes[2].contour(W, B, MSE_grid, levels=20, cmap='viridis', alpha=0.7)\n",
    "axes[2].plot(history['w'], history['b'], 'ro-', markersize=2, linewidth=1, alpha=0.7)\n",
    "axes[2].scatter([history['w'][0]], [history['b'][0]], color='green', s=100, \n",
    "                marker='o', label='Start', zorder=5)\n",
    "axes[2].scatter([history['w'][-1]], [history['b'][-1]], color='red', s=100, \n",
    "                marker='*', label='End', zorder=5)\n",
    "axes[2].set_xlabel('Weight (w)', fontsize=12)\n",
    "axes[2].set_ylabel('Bias (b)', fontsize=12)\n",
    "axes[2].set_title('üó∫Ô∏è Gradient Descent Path\\n(Walking down the mountain)', fontsize=14)\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('05_gradient_descent_progress.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüéâ Gradient Descent found values very close to the TRUE values!\")\n",
    "print(f\"   Learned w = {learned_w:.4f} (True: {true_slope})\")\n",
    "print(f\"   Learned b = {learned_b:.4f} (True: {true_intercept})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see our learned line on the data!\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(study_hours, test_scores, s=100, c='royalblue', \n",
    "            alpha=0.7, edgecolors='darkblue', linewidth=1.5, \n",
    "            label='Student Data')\n",
    "\n",
    "# Plot our learned line\n",
    "x_line = np.linspace(0, 9, 100)\n",
    "y_line = learned_w * x_line + learned_b\n",
    "plt.plot(x_line, y_line, 'r-', linewidth=3, \n",
    "         label=f'Our Line: y = {learned_w:.2f}x + {learned_b:.2f}')\n",
    "\n",
    "# Add labels\n",
    "plt.xlabel('üìö Study Hours', fontsize=14)\n",
    "plt.ylabel('üìù Test Score', fontsize=14)\n",
    "plt.title('üéì Our Linear Regression Line!', fontsize=16, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 9)\n",
    "plt.ylim(30, 105)\n",
    "\n",
    "plt.savefig('06_final_regression_line.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîÆ Now we can make PREDICTIONS!\")\n",
    "print(\"\\n   Example: If a student studies for 5 hours...\")\n",
    "prediction = learned_w * 5 + learned_b\n",
    "print(f\"   Predicted score = {learned_w:.2f} √ó 5 + {learned_b:.2f} = {prediction:.1f} points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: Learning Rate - The Step Size üë£\n",
    "\n",
    "The **learning rate (Œ±)** controls how big of a step we take during gradient descent.\n",
    "\n",
    "- **Too small**: Learning is very slow (takes forever!)\n",
    "- **Too big**: We might overshoot and miss the best point!\n",
    "- **Just right**: We find the answer efficiently\n",
    "\n",
    "It's like Goldilocks - we need to find the one that's \"just right\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how different learning rates affect training!\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1]\n",
    "colors = ['blue', 'green', 'red']\n",
    "labels = ['Too Small (0.001)', 'Just Right (0.01)', 'Too Big (0.1)']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "for i, (lr, color, label) in enumerate(zip(learning_rates, colors, labels)):\n",
    "    _, _, hist = gradient_descent(study_hours, test_scores, \n",
    "                                  learning_rate=lr, n_iterations=200, verbose=False)\n",
    "    \n",
    "    # Learning curve\n",
    "    axes[0].plot(hist['mse'], color=color, linewidth=2, label=f'Œ±={lr}')\n",
    "    \n",
    "    # Path on contour\n",
    "    axes[1].contour(W, B, MSE_grid, levels=15, cmap='Greys', alpha=0.5)\n",
    "    axes[1].plot(hist['w'], hist['b'], color=color, linewidth=2, \n",
    "                 marker='o', markersize=3, label=f'Œ±={lr}', alpha=0.7)\n",
    "\n",
    "# Plot 1: Learning curves\n",
    "axes[0].set_xlabel('Iteration', fontsize=12)\n",
    "axes[0].set_ylabel('MSE (Error)', fontsize=12)\n",
    "axes[0].set_title('üìâ Learning Curves for Different Œ±', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Paths\n",
    "axes[1].scatter([true_slope], [true_intercept], color='gold', s=200, \n",
    "                marker='*', label='Target', zorder=10)\n",
    "axes[1].set_xlabel('Weight (w)', fontsize=12)\n",
    "axes[1].set_ylabel('Bias (b)', fontsize=12)\n",
    "axes[1].set_title('üó∫Ô∏è Paths with Different Œ±', fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "# Plot 3: Visual explanation\n",
    "axes[2].axis('off')\n",
    "explanation = \"\"\"\n",
    "üéØ LEARNING RATE GUIDE:\n",
    "\n",
    "üê¢ Too Small (Œ± = 0.001):\n",
    "   ‚Ä¢ Very slow progress\n",
    "   ‚Ä¢ Safe but takes forever\n",
    "   ‚Ä¢ Like taking tiny baby steps\n",
    "\n",
    "‚úÖ Just Right (Œ± = 0.01):\n",
    "   ‚Ä¢ Fast but controlled\n",
    "   ‚Ä¢ Reaches the goal efficiently\n",
    "   ‚Ä¢ Like walking normally\n",
    "\n",
    "üèÉ Too Big (Œ± = 0.1):\n",
    "   ‚Ä¢ Very fast initially\n",
    "   ‚Ä¢ May overshoot and bounce\n",
    "   ‚Ä¢ Like trying to sprint downhill\n",
    "\n",
    "üí° TIP: Start with Œ± = 0.01 and adjust!\n",
    "\"\"\"\n",
    "axes[2].text(0.1, 0.9, explanation, fontsize=14, verticalalignment='top',\n",
    "             fontfamily='monospace', transform=axes[2].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('07_learning_rate_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Polynomial Regression - When Lines Aren't Enough! üåä\n",
    "\n",
    "Sometimes data doesn't follow a straight line. In these cases, we need **curves**!\n",
    "\n",
    "**Polynomial Regression** lets us fit curves instead of lines by using powers of x:\n",
    "\n",
    "- **Linear (degree 1)**: $y = w_1 x + b$\n",
    "- **Quadratic (degree 2)**: $y = w_2 x^2 + w_1 x + b$\n",
    "- **Cubic (degree 3)**: $y = w_3 x^3 + w_2 x^2 + w_1 x + b$\n",
    "\n",
    "Let's see this with a new example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create data that follows a curve!\n",
    "# Example: Plant growth - grows fast at first, then slows down\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Days of observation\n",
    "days = np.linspace(0, 30, 40)\n",
    "\n",
    "# Plant height follows a logarithmic/polynomial pattern\n",
    "# Height = -0.01*days^2 + 0.8*days + 2 (with noise)\n",
    "true_height = -0.01 * days**2 + 0.8 * days + 2\n",
    "noise = np.random.normal(0, 1, len(days))\n",
    "plant_height = true_height + noise\n",
    "\n",
    "# Make sure heights are positive\n",
    "plant_height = np.maximum(plant_height, 0)\n",
    "\n",
    "# Visualize this data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(days, plant_height, s=80, c='forestgreen', alpha=0.7, \n",
    "            edgecolors='darkgreen', linewidth=1.5, label='Observed Height')\n",
    "plt.xlabel('üóìÔ∏è Days', fontsize=14)\n",
    "plt.ylabel('üå± Plant Height (cm)', fontsize=14)\n",
    "plt.title('üåª Plant Growth Over Time\\n(Notice: This is NOT a straight line!)', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('08_polynomial_data.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç OBSERVATION:\")\n",
    "print(\"   This data clearly follows a CURVE, not a straight line!\")\n",
    "print(\"   A regular line would not fit well here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare different polynomial degrees!\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Prepare our data\n",
    "X = days.reshape(-1, 1)  # Reshape for sklearn\n",
    "y = plant_height\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = [1, 2, 3, 5, 10]\n",
    "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Store MSE values for comparison\n",
    "mse_values = []\n",
    "\n",
    "for idx, (degree, color) in enumerate(zip(degrees, colors)):\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Fit the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly, y)\n",
    "    \n",
    "    # Make predictions\n",
    "    X_line = np.linspace(0, 30, 100).reshape(-1, 1)\n",
    "    X_line_poly = poly.transform(X_line)\n",
    "    y_pred = model.predict(X_line_poly)\n",
    "    \n",
    "    # Calculate MSE\n",
    "    y_train_pred = model.predict(X_poly)\n",
    "    mse = calculate_mse(y, y_train_pred)\n",
    "    mse_values.append(mse)\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(days, plant_height, s=60, c='forestgreen', alpha=0.6, \n",
    "                      edgecolors='darkgreen', label='Data')\n",
    "    axes[idx].plot(X_line, y_pred, color=color, linewidth=2.5, \n",
    "                   label=f'Degree {degree}')\n",
    "    axes[idx].set_xlabel('Days')\n",
    "    axes[idx].set_ylabel('Height (cm)')\n",
    "    axes[idx].set_title(f'Polynomial Degree {degree}\\nMSE = {mse:.2f}', fontsize=12)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].set_ylim(-2, 20)\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "# Last plot: MSE comparison\n",
    "axes[5].bar(range(len(degrees)), mse_values, color=colors)\n",
    "axes[5].set_xticks(range(len(degrees)))\n",
    "axes[5].set_xticklabels([f'Degree {d}' for d in degrees])\n",
    "axes[5].set_ylabel('MSE (Error)')\n",
    "axes[5].set_title('üìä Error Comparison', fontsize=12)\n",
    "axes[5].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('üîÆ Polynomial Regression: Finding the Right Curve!', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('09_polynomial_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä OBSERVATIONS:\")\n",
    "print(\"   ‚Ä¢ Degree 1 (straight line): Doesn't capture the curve at all!\")\n",
    "print(\"   ‚Ä¢ Degree 2 (quadratic): Fits nicely! Captures the growth pattern.\")\n",
    "print(\"   ‚Ä¢ Degree 3-5: Similar to degree 2, slightly better fit.\")\n",
    "print(\"   ‚Ä¢ Degree 10: Fits training data VERY well, but...\")\n",
    "print(\"\\n‚ö†Ô∏è  WARNING: Higher degree isn't always better! See Part 8.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Overfitting - When Models Get TOO Creative! üé®\n",
    "\n",
    "**Overfitting** happens when our model learns the training data TOO well, including the noise!\n",
    "\n",
    "It's like memorizing answers instead of learning the concept - you'll do great on the practice test, but fail the real exam!\n",
    "\n",
    "Let's see what happens when we use a very high polynomial degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate overfitting with a dramatic example\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create simple training data (fewer points makes overfitting more obvious)\n",
    "X_train = np.array([1, 2, 3, 4, 5, 6, 7, 8]).reshape(-1, 1)\n",
    "y_train = np.array([2.5, 3.1, 3.8, 5.2, 5.8, 7.1, 7.5, 8.2])\n",
    "\n",
    "# Create test data (new data the model hasn't seen)\n",
    "X_test = np.array([1.5, 3.5, 5.5, 7.5]).reshape(-1, 1)\n",
    "y_test = np.array([2.8, 4.5, 6.5, 7.8])\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "X_line = np.linspace(0.5, 8.5, 100).reshape(-1, 1)\n",
    "\n",
    "scenarios = [\n",
    "    (1, 'Underfitting (Too Simple)', 'red'),\n",
    "    (2, 'Just Right! ‚úÖ', 'green'),\n",
    "    (7, 'Overfitting (Too Complex)', 'purple')\n",
    "]\n",
    "\n",
    "for ax, (degree, title, color) in zip(axes, scenarios):\n",
    "    # Fit polynomial\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    X_line_poly = poly.transform(X_line)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_line = model.predict(X_line_poly)\n",
    "    y_train_pred = model.predict(X_train_poly)\n",
    "    y_test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_mse = calculate_mse(y_train, y_train_pred)\n",
    "    test_mse = calculate_mse(y_test, y_test_pred)\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(X_train, y_train, s=100, c='blue', edgecolors='darkblue',\n",
    "               label='Training Data', zorder=5)\n",
    "    ax.scatter(X_test, y_test, s=100, c='orange', edgecolors='darkorange',\n",
    "               marker='s', label='Test Data', zorder=5)\n",
    "    ax.plot(X_line, y_line, color=color, linewidth=2.5, label=f'Degree {degree}')\n",
    "    \n",
    "    ax.set_xlabel('X', fontsize=12)\n",
    "    ax.set_ylabel('Y', fontsize=12)\n",
    "    ax.set_title(f'{title}\\n(Degree {degree})', fontsize=14)\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add MSE text\n",
    "    ax.text(0.95, 0.05, f'Train MSE: {train_mse:.2f}\\nTest MSE: {test_mse:.2f}',\n",
    "            transform=ax.transAxes, fontsize=11, verticalalignment='bottom',\n",
    "            horizontalalignment='right', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.suptitle('üéØ The Goldilocks Problem: Finding the Right Complexity!', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('10_overfitting_demo.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüîç KEY OBSERVATIONS:\")\n",
    "print(\"\\n   üìà UNDERFITTING (Degree 1):\")\n",
    "print(\"      ‚Ä¢ Too simple - can't capture the pattern\")\n",
    "print(\"      ‚Ä¢ High error on BOTH training and test data\")\n",
    "print(\"\\n   ‚úÖ JUST RIGHT (Degree 2):\")\n",
    "print(\"      ‚Ä¢ Captures the general pattern\")\n",
    "print(\"      ‚Ä¢ Good performance on BOTH training and test data\")\n",
    "print(\"\\n   üåÄ OVERFITTING (Degree 7):\")\n",
    "print(\"      ‚Ä¢ Memorizes training data (low training error)\")\n",
    "print(\"      ‚Ä¢ Fails on new data (high test error)\")\n",
    "print(\"      ‚Ä¢ Creates crazy wiggles that don't make sense!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 9: Regularization - Keeping Models Under Control! üéõÔ∏è\n",
    "\n",
    "**Regularization** is like putting training wheels on our model - it prevents it from getting too wild!\n",
    "\n",
    "There are two main types:\n",
    "\n",
    "## 1. Ridge Regression (L2 Regularization)\n",
    "- Adds a penalty based on the **square** of the weights\n",
    "- Makes weights **smaller** but keeps them all\n",
    "- Like telling the model: \"Don't make any single feature too important!\"\n",
    "\n",
    "## 2. Lasso Regression (L1 Regularization)  \n",
    "- Adds a penalty based on the **absolute value** of weights\n",
    "- Can make some weights exactly **zero** (removes features!)\n",
    "- Like telling the model: \"Only keep the most important features!\"\n",
    "\n",
    "The **regularization strength (Œ±)** controls how strong the penalty is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "# Let's see how regularization helps with our overfitting example!\n",
    "\n",
    "# Use a high degree polynomial that would normally overfit\n",
    "degree = 7\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "X_line_poly = poly.transform(X_line)\n",
    "\n",
    "# Different regularization strengths\n",
    "alphas = [0, 0.01, 0.1, 1, 10]\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
    "\n",
    "for ax, alpha in zip(axes, alphas):\n",
    "    # Use Ridge regression\n",
    "    if alpha == 0:\n",
    "        model = LinearRegression()\n",
    "    else:\n",
    "        model = Ridge(alpha=alpha)\n",
    "    \n",
    "    model.fit(X_train_poly, y_train)\n",
    "    y_line = model.predict(X_line_poly)\n",
    "    \n",
    "    # Calculate errors\n",
    "    train_mse = calculate_mse(y_train, model.predict(X_train_poly))\n",
    "    test_mse = calculate_mse(y_test, model.predict(X_test_poly))\n",
    "    \n",
    "    # Plot\n",
    "    ax.scatter(X_train, y_train, s=80, c='blue', edgecolors='darkblue', label='Train')\n",
    "    ax.scatter(X_test, y_test, s=80, c='orange', marker='s', label='Test')\n",
    "    ax.plot(X_line, y_line, 'purple', linewidth=2)\n",
    "    ax.set_title(f'Œ± = {alpha}\\nTest MSE = {test_mse:.2f}', fontsize=12)\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlabel('X')\n",
    "    if alpha == 0:\n",
    "        ax.set_ylabel('Y')\n",
    "\n",
    "plt.suptitle('üéõÔ∏è Ridge Regularization: Taming the Overfitting Monster!\\n(All using Degree 7 polynomial)', \n",
    "             fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.savefig('11_ridge_regularization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° NOTICE:\")\n",
    "print(\"   ‚Ä¢ Œ±=0: No regularization ‚Üí Wild overfitting!\")\n",
    "print(\"   ‚Ä¢ Œ±=0.01 to 1: Regularization kicks in ‚Üí Smoother curves\")\n",
    "print(\"   ‚Ä¢ Œ±=10: Too much regularization ‚Üí Almost a straight line\")\n",
    "print(\"\\n   The SWEET SPOT is usually somewhere in the middle!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare Ridge vs Lasso\n",
    "\n",
    "# Create more complex data for this demo\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "X_demo = np.sort(np.random.uniform(0, 10, n_samples)).reshape(-1, 1)\n",
    "y_demo = 0.5 * X_demo.ravel()**2 - 2*X_demo.ravel() + 3 + np.random.normal(0, 3, n_samples)\n",
    "\n",
    "# High degree polynomial\n",
    "degree = 10\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X_demo_poly = poly.fit_transform(X_demo)\n",
    "\n",
    "X_line = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "X_line_poly = poly.transform(X_line)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# No regularization\n",
    "model_none = LinearRegression()\n",
    "model_none.fit(X_demo_poly, y_demo)\n",
    "y_none = model_none.predict(X_line_poly)\n",
    "\n",
    "# Ridge\n",
    "model_ridge = Ridge(alpha=10)\n",
    "model_ridge.fit(X_demo_poly, y_demo)\n",
    "y_ridge = model_ridge.predict(X_line_poly)\n",
    "\n",
    "# Lasso\n",
    "model_lasso = Lasso(alpha=1, max_iter=10000)\n",
    "model_lasso.fit(X_demo_poly, y_demo)\n",
    "y_lasso = model_lasso.predict(X_line_poly)\n",
    "\n",
    "# Plot comparison\n",
    "models = [\n",
    "    (y_none, model_none.coef_, 'No Regularization', 'red'),\n",
    "    (y_ridge, model_ridge.coef_, 'Ridge (L2)', 'blue'),\n",
    "    (y_lasso, model_lasso.coef_, 'Lasso (L1)', 'green')\n",
    "]\n",
    "\n",
    "for ax, (y_pred, coefs, title, color) in zip(axes, models):\n",
    "    ax.scatter(X_demo, y_demo, s=60, c='gray', alpha=0.6, label='Data')\n",
    "    ax.plot(X_line, y_pred, color=color, linewidth=2.5, label='Prediction')\n",
    "    ax.set_xlabel('X', fontsize=12)\n",
    "    ax.set_ylabel('Y', fontsize=12)\n",
    "    ax.set_title(f'{title}\\nNon-zero coefficients: {np.sum(np.abs(coefs) > 0.01)}', fontsize=12)\n",
    "    ax.set_ylim(-10, 50)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('üî¨ Comparing Regularization Methods (Degree 10 Polynomial)', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('12_regularization_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the coefficients (weights) for each method\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(model_none.coef_))\n",
    "width = 0.25\n",
    "\n",
    "# Limit coefficient magnitudes for visualization\n",
    "coef_none = np.clip(model_none.coef_, -50, 50)\n",
    "coef_ridge = model_ridge.coef_\n",
    "coef_lasso = model_lasso.coef_\n",
    "\n",
    "bars1 = ax.bar(x_pos - width, coef_none, width, label='No Regularization', color='red', alpha=0.7)\n",
    "bars2 = ax.bar(x_pos, coef_ridge, width, label='Ridge (L2)', color='blue', alpha=0.7)\n",
    "bars3 = ax.bar(x_pos + width, coef_lasso, width, label='Lasso (L1)', color='green', alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Coefficient Index (x‚Å∞, x¬π, x¬≤, ...)', fontsize=12)\n",
    "ax.set_ylabel('Coefficient Value', fontsize=12)\n",
    "ax.set_title('üìä How Regularization Affects Coefficients', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'x{i}' for i in range(len(coef_none))])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(y=0, color='black', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('13_coefficient_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä COEFFICIENT ANALYSIS:\")\n",
    "print(f\"\\n   No Regularization: {np.sum(np.abs(model_none.coef_) > 0.01)} non-zero coefficients\")\n",
    "print(f\"   Ridge (L2):        {np.sum(np.abs(model_ridge.coef_) > 0.01)} non-zero coefficients (all small)\")\n",
    "print(f\"   Lasso (L1):        {np.sum(np.abs(model_lasso.coef_) > 0.01)} non-zero coefficients (some eliminated!)\")\n",
    "\n",
    "print(\"\\nüí° KEY DIFFERENCES:\")\n",
    "print(\"   ‚Ä¢ Ridge: Shrinks ALL coefficients but keeps them\")\n",
    "print(\"   ‚Ä¢ Lasso: Sets some coefficients to EXACTLY zero (feature selection!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 10: Putting It All Together - Complete Example! üéâ\n",
    "\n",
    "Let's use everything we learned on a real-world-style example: **Predicting House Prices!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a realistic house price dataset\n",
    "np.random.seed(42)\n",
    "n_houses = 100\n",
    "\n",
    "# Features\n",
    "size_sqft = np.random.uniform(500, 3500, n_houses)  # House size in square feet\n",
    "\n",
    "# Price depends on size (roughly $150 per sqft) plus base cost, with some noise\n",
    "true_price = 50000 + 150 * size_sqft + np.random.normal(0, 30000, n_houses)\n",
    "true_price = np.maximum(true_price, 50000)  # Minimum price\n",
    "\n",
    "# Create dataframe\n",
    "house_data = pd.DataFrame({\n",
    "    'Size (sqft)': np.round(size_sqft, 0),\n",
    "    'Price ($)': np.round(true_price, -3)  # Round to nearest thousand\n",
    "})\n",
    "\n",
    "print(\"üè† HOUSE PRICE DATASET\")\n",
    "print(\"=\" * 40)\n",
    "print(house_data.head(10).to_string(index=False))\n",
    "print(f\"\\n... and {n_houses - 10} more houses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X = size_sqft.reshape(-1, 1)\n",
    "y = true_price\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"üìä Data Split:\")\n",
    "print(f\"   Training set: {len(X_train)} houses\")\n",
    "print(f\"   Testing set:  {len(X_test)} houses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different models and compare!\n",
    "\n",
    "# 1. Simple Linear Regression\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X_train, y_train)\n",
    "\n",
    "# 2. Polynomial Regression (degree 2)\n",
    "poly2 = PolynomialFeatures(degree=2)\n",
    "X_train_poly2 = poly2.fit_transform(X_train)\n",
    "X_test_poly2 = poly2.transform(X_test)\n",
    "model_poly2 = LinearRegression()\n",
    "model_poly2.fit(X_train_poly2, y_train)\n",
    "\n",
    "# 3. Ridge Regression (with polynomial features)\n",
    "poly3 = PolynomialFeatures(degree=3)\n",
    "X_train_poly3 = poly3.fit_transform(X_train)\n",
    "X_test_poly3 = poly3.transform(X_test)\n",
    "model_ridge = Ridge(alpha=1000)  # Strong regularization\n",
    "model_ridge.fit(X_train_poly3, y_train)\n",
    "\n",
    "# Evaluate all models\n",
    "def evaluate_model(name, model, X_tr, X_te, y_tr, y_te):\n",
    "    train_pred = model.predict(X_tr)\n",
    "    test_pred = model.predict(X_te)\n",
    "    train_mse = calculate_mse(y_tr, train_pred)\n",
    "    test_mse = calculate_mse(y_te, test_pred)\n",
    "    return train_mse, test_mse\n",
    "\n",
    "results = [\n",
    "    ('Linear Regression', *evaluate_model('Linear', model_linear, X_train, X_test, y_train, y_test)),\n",
    "    ('Polynomial (deg 2)', *evaluate_model('Poly2', model_poly2, X_train_poly2, X_test_poly2, y_train, y_test)),\n",
    "    ('Ridge + Poly (deg 3)', *evaluate_model('Ridge', model_ridge, X_train_poly3, X_test_poly3, y_train, y_test))\n",
    "]\n",
    "\n",
    "print(\"\\nüèÜ MODEL COMPARISON:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<25} {'Train MSE':>15} {'Test MSE':>15}\")\n",
    "print(\"-\" * 60)\n",
    "for name, train_mse, test_mse in results:\n",
    "    print(f\"{name:<25} {train_mse:>15,.0f} {test_mse:>15,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization!\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Prepare line data\n",
    "X_line = np.linspace(400, 3600, 100).reshape(-1, 1)\n",
    "\n",
    "# Left plot: All models comparison\n",
    "axes[0].scatter(X_train, y_train, s=60, c='blue', alpha=0.5, label='Training Data')\n",
    "axes[0].scatter(X_test, y_test, s=60, c='orange', alpha=0.5, marker='s', label='Test Data')\n",
    "\n",
    "# Linear prediction\n",
    "axes[0].plot(X_line, model_linear.predict(X_line), 'r-', linewidth=2, label='Linear')\n",
    "\n",
    "# Polynomial prediction\n",
    "axes[0].plot(X_line, model_poly2.predict(poly2.transform(X_line)), 'g-', linewidth=2, label='Polynomial')\n",
    "\n",
    "# Ridge prediction\n",
    "axes[0].plot(X_line, model_ridge.predict(poly3.transform(X_line)), 'purple', linewidth=2, \n",
    "             linestyle='--', label='Ridge')\n",
    "\n",
    "axes[0].set_xlabel('üè† House Size (sqft)', fontsize=12)\n",
    "axes[0].set_ylabel('üí∞ Price ($)', fontsize=12)\n",
    "axes[0].set_title('All Models Comparison', fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "\n",
    "# Right plot: Predictions vs Actual\n",
    "test_pred = model_linear.predict(X_test)\n",
    "axes[1].scatter(y_test, test_pred, s=80, c='green', alpha=0.7, edgecolors='darkgreen')\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "             'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('üí∞ Actual Price ($)', fontsize=12)\n",
    "axes[1].set_ylabel('üîÆ Predicted Price ($)', fontsize=12)\n",
    "axes[1].set_title('Predictions vs Actual (Linear Model)', fontsize=14)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "axes[1].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x/1000:.0f}K'))\n",
    "\n",
    "plt.suptitle('üè† House Price Prediction Results!', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('14_final_house_price_model.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Make a prediction!\n",
    "new_house_size = 2000\n",
    "predicted_price = model_linear.predict([[new_house_size]])[0]\n",
    "print(f\"\\nüîÆ PREDICTION EXAMPLE:\")\n",
    "print(f\"   A house that is {new_house_size} sqft would cost approximately ${predicted_price:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéì Congratulations! Summary of What You Learned\n",
    "\n",
    "## Key Concepts Covered:\n",
    "\n",
    "### 1. **Linear Regression Basics**\n",
    "- Finding the best line: $\\hat{y} = wx + b$\n",
    "- **w (weight)** controls the slope\n",
    "- **b (bias)** controls where the line starts\n",
    "\n",
    "### 2. **Cost Function (MSE)**\n",
    "- Measures how \"wrong\" our predictions are\n",
    "- Lower MSE = Better model!\n",
    "\n",
    "### 3. **Gradient Descent**\n",
    "- How computers \"learn\" by taking steps toward lower error\n",
    "- **Learning rate** controls step size\n",
    "- Too small = slow, Too big = unstable\n",
    "\n",
    "### 4. **Polynomial Regression**\n",
    "- When data follows curves, not lines\n",
    "- Higher degree = More complex curves\n",
    "\n",
    "### 5. **Overfitting**\n",
    "- When models memorize instead of learn\n",
    "- Good on training data, bad on new data\n",
    "\n",
    "### 6. **Regularization**\n",
    "- **Ridge (L2)**: Shrinks all weights\n",
    "- **Lasso (L1)**: Removes some features entirely\n",
    "- Helps prevent overfitting!\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps to Continue Learning:\n",
    "\n",
    "1. **Try different datasets** - Find data online and practice!\n",
    "2. **Experiment with hyperparameters** - Change learning rates, polynomial degrees, regularization strength\n",
    "3. **Learn about multiple features** - What if we use both house size AND number of bedrooms?\n",
    "4. **Explore other algorithms** - Decision trees, neural networks, and more!\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Learning! üéâ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Reference Card - Save this!\n",
    "\n",
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                    üìö LINEAR REGRESSION CHEAT SHEET                  ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  THE EQUATION:  ≈∑ = w¬∑x + b                                          ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  COST FUNCTION: MSE = (1/n) Œ£(actual - predicted)¬≤                   ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  GRADIENT DESCENT:                                                   ‚ïë\n",
    "‚ïë     w_new = w_old - Œ± ¬∑ (‚àÇMSE/‚àÇw)                                    ‚ïë\n",
    "‚ïë     b_new = b_old - Œ± ¬∑ (‚àÇMSE/‚àÇb)                                    ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  LEARNING RATE (Œ±):                                                  ‚ïë\n",
    "‚ïë     Too small ‚Üí Slow learning                                        ‚ïë\n",
    "‚ïë     Too big   ‚Üí Unstable/overshooting                                ‚ïë\n",
    "‚ïë     Just right ‚Üí Fast & stable                                       ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  POLYNOMIAL REGRESSION:                                              ‚ïë\n",
    "‚ïë     Degree 1 ‚Üí Line                                                  ‚ïë\n",
    "‚ïë     Degree 2 ‚Üí Parabola                                              ‚ïë\n",
    "‚ïë     Degree n ‚Üí Complex curve                                         ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  REGULARIZATION:                                                     ‚ïë\n",
    "‚ïë     Ridge (L2) ‚Üí Shrinks weights                                     ‚ïë\n",
    "‚ïë     Lasso (L1) ‚Üí Removes features                                    ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïë  GOLDEN RULES:                                                       ‚ïë\n",
    "‚ïë     ‚úì Start simple, add complexity if needed                         ‚ïë\n",
    "‚ïë     ‚úì Always check performance on TEST data                          ‚ïë\n",
    "‚ïë     ‚úì Use regularization to prevent overfitting                      ‚ïë\n",
    "‚ïë     ‚úì Visualize your data and predictions!                           ‚ïë\n",
    "‚ïë                                                                      ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
